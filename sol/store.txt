package sol

import src.{FileIO, PorterStemmer, StopWords}
import scala.collection.mutable
import scala.util.matching.Regex
import scala.xml.{Node, NodeSeq}

/**
 * Provides an XML indexer, produces files for a querier
 *
 * @param inputFile - the filename of the XML wiki to be indexed
 */
class Indexer(val inputFile: String) {
  //These variables are needed to compute term frequency
  //Number of pages in the corpus or n
  var numOfPages = 0.0
  //Number of words in the corpus
  var numOfWords = 0

  //These HashMaps are used to compute page rank
  //Unranked documents in the corpus
  val rMap = new mutable.HashMap[Int, Double]()
  //Corpus documents ranked with PageRank
  val rPrimeMap = new mutable.HashMap[Int, Double]()
  //All page ids mapped to the pages they link to
  val linksMap = new mutable.HashMap[Int, List[Int]]()

  //Theses HashMaps link the titles and ids to each other in two HashMaps
  val idToTitle = new mutable.HashMap[Int, String]()
  val titleToId = new mutable.HashMap[String, Int]()

  // List of Ids and Titles
  val idList: List[Int] = idToTitle.keys.toList
  val titlesList: List[String] = titleToId.keys.toList
  //HashMap of words -> (page Id -> Term Frequency)
  val wordsMap = new mutable.HashMap[String,
    mutable.HashMap[Int, Double]]()

  //Accessing the xml file
  //Node containing entire input file
  val root: Node = xml.XML.loadFile(inputFile)
  //Sequence of pages extracted from the root node
  val pageSeq: NodeSeq = root \ "page"
  //Sequence of ids extracted from the root node
  val idSeq: NodeSeq = root \ "page" \ "id"
  //Sequence of titles extracted from the root node
  val titleSeq: NodeSeq = root \ "page" \ "title"

  //Begin indexing and page rank
  parse(inputFile)
  relevance()
  pageRank()




  /**
   *
   * @param lst
   * @param pageId
   * @param pageTitle
   */
  def linkHelper(lst: List[String], pageId: Int, pageTitle: String): List[String] = {
    //Words from links that appear on the page
    var linkTextWords: List[String] = List()
    for (x <- lst) {
      if (x.matches("""\|+""") && x.matches(""":+""")) {
        val linksTo = x.substring(0, x.indexOf("|") - 1)
        if (titlesList.contains(linksTo) && !linksTo.equals(pageTitle)) {
          if (linksMap.keys.toList.contains(pageId)) {
            linksMap.update(pageId, titleToId(linksTo) :: linksMap(pageId))
          } else {
            linksMap.put(pageId, List(titleToId(linksTo)))
          }
          //Remove all : and | after from text that appears on the page
          val rem = x.substring(x.indexOf("|") + 1, x.length)
            .replaceAll(""":+|\|+""", " ")
          val remArray = rem.split("""\s+""")
          for (word <- remArray) {
            linkTextWords = word :: linkTextWords
          }
        }
      } else if (x.matches(""":+""")) {
        //Remove all : from text that appears on the page
        val rem = x.replaceAll(""":+""", " ")
        val remArray = rem.split("""\s+""")
        for (word <- remArray) {
          linkTextWords = word :: linkTextWords
        }
      } else if (x.matches("""\|+""")) {
        val linksTo = x.substring(0, x.indexOf("""|+""") - 1)
        if (titlesList.contains(linksTo) && !linksTo.equals(pageTitle)) {
          if (linksMap.keys.toList.contains(pageId)) {
            linksMap.update(pageId, titleToId(linksTo) :: linksMap(pageId))
          } else {
            linksMap.put(pageId, List(titleToId(linksTo)))
          }
        }
        //Remove all | from text that appears on the page
        val rem = x.substring(x.indexOf("|") + 1, x.length)
          .replaceAll("""\|+""", " ")
        val remArray = rem.split("""\s+""")
        for (word <- remArray) {
          linkTextWords = word :: linkTextWords
        }
      } else {
        //Link has no : or |
        val remArray = x.split(""""\s+""")
        for (word <- remArray) {
          linkTextWords = word :: linkTextWords
        }
      }
    }
    linkTextWords
  }

  /**
   * Parse method constructs [insert structures here]
   *
   * @param inputFile
   */
  def parse(inputFile: String): Unit = {
//    for (page <- pageSeq) {
//      val pageTitle = (page \ "title").text.trim
//      val pageId = (page \ "id").text.trim.toInt
//      //Store ID to title
//      idToTitle.put(pageId, pageTitle)
//      //Store title to ID
//      titleToId.put(pageTitle, pageId)
//    }
    for (page <- pageSeq) {
      numOfPages += 1
      //Grab the id, title, and text from the current page
      val pageId = (page \ "id").text.trim.toInt
      val pageTitle = (page \ "title").text.trim
      val pageText = (page \ "text").text.trim

      //Extract all the words and links from the pages
      val genRegex = new Regex("""\[\[[^\[]+?\]\]|[^\W_]+'[^\W_]+|[^\W_]+""")
      val matchesIterator = genRegex.findAllMatchIn(pageTitle + pageText)
      val matchesList = matchesIterator.toList.map { aMatch => aMatch.matched }

      //Process the words and links
      val pageLinks = matchesList.filter(x => x.matches("""\[\[.*\]\]"""))
      val linkList = pageLinks.map(x => x.stripPrefix("[[").stripSuffix("]]"))
      //Add all link names to the Links HashMap and all words to the word list
      //to be processed
      val linkTextWords = linkHelper(linkList, pageId, pageTitle)

      //Continue processing the words
      val pageWords = matchesList.filter(x => !x.matches("""\[\[.*\]\]"""))
      val noStop = pageWords.filterNot(x => StopWords.isStopWord(x))
      val lowerCaseNoStop = noStop.map(x => x.toLowerCase)
      val filteredWords = lowerCaseNoStop.map(x => PorterStemmer.stem(x))
      for (word <- filteredWords) {
        numOfWords += 1
        if (wordsMap.keys.toList.contains(word)) {
          if (wordsMap(word).contains(pageId)) {
            wordsMap(word)(pageId) += 1
          } else {
            wordsMap(word).put(pageId, 1)
          }
        } else {
          wordsMap.put(word, new mutable.HashMap(pageId, 1))
        }
      }
      val noStopLinks = linkTextWords.filterNot(x => StopWords.isStopWord(x))
      val lowerCaseNoStopLinks = noStopLinks.map(x => x.toLowerCase)
      val filteredLinkWords = lowerCaseNoStopLinks.map(x => PorterStemmer.stem(x))
      for (word <- filteredLinkWords) {
        numOfWords += 1
        if (wordsMap.keys.toList.contains(word)) {
          if (wordsMap(word).contains(pageId)) {
            wordsMap(word)(pageId) += 1
          } else {
            wordsMap(word).put(pageId, 1)
          }
        } else {
          wordsMap.put(word, new mutable.HashMap(pageId, 1))
        }
      }
      //For loop ends
    }
    //Parse ends
  }


  //Calculate relevance and store it in the outputWords HashMap
  //First, calculate term frequency
  def relevance(): Unit = {
    val occurrenceMap = new mutable.HashMap[Int, Double]()
    for ((_, map) <- wordsMap) {
      for (id <- map.keys) {
        if (occurrenceMap.keys.toList.contains(id)) {
          if (map(id) > occurrenceMap(id)) {
            occurrenceMap.update(id, map(id))
          }
          occurrenceMap.put(id, map(id))
        }
      }
    }
    //Using term frequency and inverse document frequency,
    // calculate relevance
    for ((word, map) <- wordsMap) {
      for (id <- map.keys) {
        map.update(id,
          (map(id) / occurrenceMap(id)) * Math.log(numOfPages / wordsMap(word).size))
      }
    }
  }

  /**
   * Calculates the weight that k (from) give to j (to)'s rank
   *
   * @param fromK - Page k
   * @param toJ   - Page j
   */
  def weight(fromK: Integer, toJ: Integer): Double = {
    if (linksMap.contains(fromK) && linksMap(fromK).contains(idToTitle(toJ))) {
      (.15 / numOfPages) + (.85 / linksMap(fromK).size.toDouble)
    } else if (linksMap.contains(fromK) && linksMap(fromK).isEmpty) {
      //(.15 / numOfPages) + (.85 / (numOfPages - 1))
      .15 / numOfPages
    } else {
      .15 / numOfPages
    }
  }

  /**
   * Page rank helper that fills a given HashMap with a certain value
   *
   * @param value
   * @param from
   * @param to
   */
  def fillHashMap(value: Double, from: mutable.HashMap[Int, String],
                  to: mutable.HashMap[Int, Double]): Unit = {
    for (ids <- from.keys) {
      to.put(ids, value)
    }
  }

  /**
   *
   * @param r
   * @param rprime
   *
   * To calculate distance we need to get the sum of the difference between r
   * and r' on every page of the hashMap
   */
  def rankDistance(r: mutable.HashMap[Int, Double],
                   rprime: mutable.HashMap[Int, Double]): Double = {
    var sum = 0.0
    for (i <- idToTitle.keys) {
      sum = sum + ((rprime(i) - r(i)) * (rprime(i) - r(i)))
    }
    Math.sqrt(sum)
  }

  def pageRank(): Unit = {
    fillHashMap(0.0, idToTitle, rMap)
    fillHashMap(1 / numOfPages, idToTitle, rPrimeMap)
    //prevIt is r and currIt is rprime

    while (rankDistance(rMap, rPrimeMap) > .001) {
      for (page <- idToTitle.keys) {
        rMap.update(page, rPrimeMap(page))
      }
      for (j <- idToTitle.keys) {
        rPrimeMap.update(j, 0.0)

        for (k <- idToTitle.keys) {
          rPrimeMap(j) += weight(k, j) * rMap(k)
          //println(weight(k, j))
        }
      }
    }
  }
}

object Indexer {
  /**
   * Processes a corpus and writes to index files.
   *
   * @param args args of the form [WikiFile, titlesIndex, docsIndex, wordsIndex]
   */
  def main(args: Array[String]): Unit = {
    if (!(args.length == 4)) {
      throw new IllegalArgumentException("Incorrect Args Config")
    }
    val indexer = new Indexer(args(0))
    FileIO.writeTitlesFile(args(1), indexer.idToTitle)
    FileIO.writeWordsFile(args(3), indexer.wordsMap)
    FileIO.writeDocsFile(args(2), indexer.rPrimeMap)
  }
}

